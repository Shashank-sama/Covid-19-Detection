{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "VGGunet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.11 64-bit ('MPF': conda)"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python",
      "version": "3.8.11"
    },
    "interpreter": {
      "hash": "3dd217612e3efbc53bda62975acccf69dc94b7f474970d20d9b0bbb03e9d7037"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "outputs": [],
      "metadata": {
        "id": "EdNjs4ow2AO0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e0ca967-0b8d-49bf-dd41-7396e15e1c3f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import pandas as pd\r\n",
        "from tqdm import tqdm\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.applications import *\r\n",
        "from tensorflow.keras.layers import *\r\n",
        "from tensorflow.keras.callbacks import *\r\n",
        "from tensorflow.keras.metrics import *\r\n",
        "from tensorflow.keras.callbacks import TensorBoard\r\n",
        "\r\n",
        "from utils.preprocess import *\r\n",
        "from utils.metrics import *"
      ],
      "outputs": [],
      "metadata": {
        "id": "OyYAnXLr2J2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data\r\n"
      ],
      "metadata": {
        "id": "0_LYIrk32NS5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def labels_map(train_labels,dev_labels,test_labels):\r\n",
        "    y_train=[]\r\n",
        "    for label in np.expand_dims(train_labels,axis=-1):\r\n",
        "      label_conc=np.concatenate([label,label],axis=-1)\r\n",
        "      y_train.append(label_conc)\r\n",
        "    y_dev=[]\r\n",
        "    for label in np.expand_dims(dev_labels,axis=-1):\r\n",
        "      label_conc=np.concatenate([label,label],axis=-1)\r\n",
        "      y_dev.append(label_conc)\r\n",
        "    y_test=[]\r\n",
        "    for label in np.expand_dims(test_labels,axis=-1):\r\n",
        "      label_conc=np.concatenate([label,label],axis=-1)\r\n",
        "      y_test.append(label_conc)\r\n",
        "    return np.array(y_train),np.array(y_dev),np.array(y_test)\r\n",
        "\r\n",
        "def parse(y_pred):\r\n",
        "    y_pred = np.expand_dims(y_pred, axis=-1)\r\n",
        "    y_pred = y_pred[..., -1]\r\n",
        "    y_pred = y_pred.astype(np.float32)\r\n",
        "    y_pred = np.expand_dims(y_pred, axis=-1)\r\n",
        "    return y_pred\r\n",
        "\r\n",
        "def evaluate_normal(model, x_data, y_data):\r\n",
        "    THRESHOLD = 0.5\r\n",
        "    total = []\r\n",
        "    for i, (x, y) in tqdm(enumerate(zip(x_data, y_data)), total=len(x_data)):\r\n",
        "        h, w, _ = x.shape\r\n",
        "\r\n",
        "        #y_pred1 = parse(model.predict(np.expand_dims(x,axis=0))[0][..., -2])\r\n",
        "        y_pred2 = parse(model.predict(np.expand_dims(x,axis=0))[0][..., -1])\r\n",
        "        \r\n",
        "        line = np.ones((h, 10,3)) * 255.0\r\n",
        "        #print(mask_to_3d(y).shape)\r\n",
        "        all_images = [\r\n",
        "            mask_to_3d(x),line, \r\n",
        "            mask_to_3d(y)* 255.0,line,\r\n",
        "            mask_to_3d(y_pred2)*255.0#,line, \r\n",
        "            #mask_to_3d(y_pred2) \r\n",
        "        ]\r\n",
        "        mask = np.concatenate(all_images, axis=1)\r\n",
        "\r\n",
        "        plt.imshow(mask,cmap='gray')\r\n",
        "        plt.show()\r\n",
        "        #cv2.imwrite(f\"results/{i}.png\", mask)\r\n",
        "def mask_to_3d(mask):   \r\n",
        "    mask = mask[:,:,0]\r\n",
        "    mask=np.expand_dims(mask,axis=-1)\r\n",
        "    mask=np.concatenate([mask,mask,mask],axis=-1)\r\n",
        "    #print(mask.shape)\r\n",
        "    return mask"
      ],
      "outputs": [],
      "metadata": {
        "id": "hBVP2Is22RB7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X_train = np.load('X_M+J+N_train.npy')\r\n",
        "X_dev = np.load('X_M+J+N_dev.npy')\r\n",
        "Y_train = np.load('y_M+J+N_train.npy')\r\n",
        "Y_dev = np.load('y_M+J+N_dev.npy')"
      ],
      "outputs": [],
      "metadata": {
        "id": "NbORhrNL8fP9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(len(X_train))\r\n",
        "print(len(X_dev))"
      ],
      "outputs": [],
      "metadata": {
        "id": "sKfHG1PylhRn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0f65ae0-95ac-403e-ba99-056c1a697b2a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "Y_train = Y_train.astype('float32')\r\n",
        "Y_dev = Y_dev.astype('float32')"
      ],
      "outputs": [],
      "metadata": {
        "id": "LpVkLxkRE8tQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X_train=min_max_preprocessing(X_train)\r\n",
        "X_dev=min_max_preprocessing(X_dev)"
      ],
      "outputs": [],
      "metadata": {
        "id": "MT2mJIA08wgx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X_train,mean,std=samplewise_preprocessing(X_train)\r\n",
        "X_dev=featurewise_preprocessing(X_dev,mean,std)"
      ],
      "outputs": [],
      "metadata": {
        "id": "x9AnVIUp9UWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "5qPDBF2227CG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Double Unet\r\n",
        "def squeeze_excite_block(inputs, ratio=8):\r\n",
        "    init = inputs\r\n",
        "    channel_axis = -1\r\n",
        "    filters = init.shape[channel_axis]\r\n",
        "    se_shape = (1, 1, filters)\r\n",
        "\r\n",
        "    se = GlobalAveragePooling2D()(init)\r\n",
        "    se = Reshape(se_shape)(se)\r\n",
        "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\r\n",
        "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\r\n",
        "\r\n",
        "    x = Multiply()([init, se])\r\n",
        "    return x\r\n",
        "\r\n",
        "def conv_block(inputs, filters):\r\n",
        "    x = inputs\r\n",
        "\r\n",
        "    x = Conv2D(filters, (3, 3), padding=\"same\")(x)\r\n",
        "    x = BatchNormalization()(x)\r\n",
        "    x = Activation('relu')(x)\r\n",
        "\r\n",
        "    x = Conv2D(filters, (3, 3), padding=\"same\")(x)\r\n",
        "    x = BatchNormalization()(x)\r\n",
        "    x = Activation('relu')(x)\r\n",
        "\r\n",
        "    x = squeeze_excite_block(x)\r\n",
        "\r\n",
        "    return x\r\n",
        "\r\n",
        "def encoder1(inputs):\r\n",
        "    skip_connections = []\r\n",
        "\r\n",
        "    model = VGG19(include_top=False, weights='imagenet', input_tensor=inputs)\r\n",
        "    names = [\"block1_conv2\", \"block2_conv2\", \"block3_conv4\", \"block4_conv4\"]\r\n",
        "    for name in names:\r\n",
        "        skip_connections.append(model.get_layer(name).output)\r\n",
        "\r\n",
        "    output = model.get_layer(\"block5_conv4\").output\r\n",
        "    return output, skip_connections\r\n",
        "\r\n",
        "def decoder1(inputs, skip_connections):\r\n",
        "    num_filters = [256, 128, 64, 32]\r\n",
        "    skip_connections.reverse()\r\n",
        "    x = inputs\r\n",
        "\r\n",
        "    for i, f in enumerate(num_filters):\r\n",
        "        x = UpSampling2D((2, 2), interpolation='bilinear')(x)\r\n",
        "        x = Concatenate()([x, skip_connections[i]])\r\n",
        "        x = conv_block(x, f)\r\n",
        "\r\n",
        "    return x\r\n",
        "\r\n",
        "# def encoder2(inputs):\r\n",
        "#     skip_connections = []\r\n",
        "#\r\n",
        "#     output = DenseNet121(include_top=False, weights='imagenet')(inputs)\r\n",
        "#     model = tf.keras.models.Model(inputs, output)\r\n",
        "#\r\n",
        "#     names = [\"input_2\", \"conv1/relu\", \"pool2_conv\", \"pool3_conv\"]\r\n",
        "#     for name in names:\r\n",
        "#         skip_connections.append(model.get_layer(name).output)\r\n",
        "#     output = model.get_layer(\"pool4_conv\").output\r\n",
        "#\r\n",
        "#     return output, skip_connections\r\n",
        "\r\n",
        "def encoder2(inputs):\r\n",
        "    num_filters = [32, 64, 128, 256]\r\n",
        "    skip_connections = []\r\n",
        "    x = inputs\r\n",
        "\r\n",
        "    for i, f in enumerate(num_filters):\r\n",
        "        x = conv_block(x, f)\r\n",
        "        skip_connections.append(x)\r\n",
        "        x = MaxPool2D((2, 2))(x)\r\n",
        "\r\n",
        "    return x, skip_connections\r\n",
        "\r\n",
        "def decoder2(inputs, skip_1, skip_2):\r\n",
        "    num_filters = [256, 128, 64, 32]\r\n",
        "    skip_2.reverse()\r\n",
        "    x = inputs\r\n",
        "\r\n",
        "    for i, f in enumerate(num_filters):\r\n",
        "        x = UpSampling2D((2, 2), interpolation='bilinear')(x)\r\n",
        "        x = Concatenate()([x, skip_1[i], skip_2[i]])\r\n",
        "        x = conv_block(x, f)\r\n",
        "\r\n",
        "    return x\r\n",
        "\r\n",
        "def output_block(inputs):\r\n",
        "    x = Conv2D(1, (1, 1), padding=\"same\")(inputs)\r\n",
        "    x = Activation('sigmoid')(x)\r\n",
        "    return x\r\n",
        "\r\n",
        "def Upsample(tensor, size):\r\n",
        "    \"\"\"Bilinear upsampling\"\"\"\r\n",
        "    def _upsample(x, size):\r\n",
        "        return tf.image.resize(images=x, size=size)\r\n",
        "    return Lambda(lambda x: _upsample(x, size), output_shape=size)(tensor)\r\n",
        "\r\n",
        "def ASPP(x, filter):\r\n",
        "    shape = x.shape\r\n",
        "\r\n",
        "    y1 = AveragePooling2D(pool_size=(shape[1], shape[2]))(x)\r\n",
        "    y1 = Conv2D(filter, 1, padding=\"same\")(y1)\r\n",
        "    y1 = BatchNormalization()(y1)\r\n",
        "    y1 = Activation(\"relu\")(y1)\r\n",
        "    y1 = UpSampling2D((shape[1], shape[2]), interpolation='bilinear')(y1)\r\n",
        "\r\n",
        "    y2 = Conv2D(filter, 1, dilation_rate=1, padding=\"same\", use_bias=False)(x)\r\n",
        "    y2 = BatchNormalization()(y2)\r\n",
        "    y2 = Activation(\"relu\")(y2)\r\n",
        "\r\n",
        "    y3 = Conv2D(filter, 3, dilation_rate=6, padding=\"same\", use_bias=False)(x)\r\n",
        "    y3 = BatchNormalization()(y3)\r\n",
        "    y3 = Activation(\"relu\")(y3)\r\n",
        "\r\n",
        "    y4 = Conv2D(filter, 3, dilation_rate=12, padding=\"same\", use_bias=False)(x)\r\n",
        "    y4 = BatchNormalization()(y4)\r\n",
        "    y4 = Activation(\"relu\")(y4)\r\n",
        "\r\n",
        "    y5 = Conv2D(filter, 3, dilation_rate=18, padding=\"same\", use_bias=False)(x)\r\n",
        "    y5 = BatchNormalization()(y5)\r\n",
        "    y5 = Activation(\"relu\")(y5)\r\n",
        "\r\n",
        "    y = Concatenate()([y1, y2, y3, y4, y5])\r\n",
        "\r\n",
        "    y = Conv2D(filter, 1, dilation_rate=1, padding=\"same\", use_bias=False)(y)\r\n",
        "    y = BatchNormalization()(y)\r\n",
        "    y = Activation(\"relu\")(y)\r\n",
        "\r\n",
        "    return y\r\n",
        "\r\n",
        "def build_model(shape):\r\n",
        "    inputs = Input(shape)\r\n",
        "    x, skip_1 = encoder1(inputs)\r\n",
        "    x = ASPP(x, 64)\r\n",
        "    x = decoder1(x, skip_1)\r\n",
        "    outputs1 = output_block(x)\r\n",
        "\r\n",
        "    x = inputs * outputs1\r\n",
        "\r\n",
        "    x, skip_2 = encoder2(x)\r\n",
        "    x = ASPP(x, 64)\r\n",
        "    x = decoder2(x, skip_1, skip_2)\r\n",
        "    outputs2 = output_block(x)\r\n",
        "    #outputs = Concatenate()([outputs1, outputs2])\r\n",
        "\r\n",
        "    model = Model(inputs, outputs2)\r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "YNOc94_PBpYN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def Unet_1():\r\n",
        "  inputs = Input(shape=(224, 224, 1), name='input')\r\n",
        "  #s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\r\n",
        "\r\n",
        "  #Contraction path\r\n",
        "  c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\r\n",
        "  c1 = tf.keras.layers.Dropout(0.1)(c1)\r\n",
        "  c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\r\n",
        "  p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\r\n",
        "\r\n",
        "  c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\r\n",
        "  c2 = tf.keras.layers.Dropout(0.1)(c2)\r\n",
        "  c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\r\n",
        "  p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\r\n",
        "  \r\n",
        "  c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\r\n",
        "  c3 = tf.keras.layers.Dropout(0.2)(c3)\r\n",
        "  c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\r\n",
        "  p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\r\n",
        "  \r\n",
        "  c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\r\n",
        "  c4 = tf.keras.layers.Dropout(0.2)(c4)\r\n",
        "  c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\r\n",
        "  p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\r\n",
        "  \r\n",
        "  c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\r\n",
        "  c5 = tf.keras.layers.Dropout(0.3)(c5)\r\n",
        "  c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\r\n",
        "\r\n",
        "  #Expansive path \r\n",
        "  u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\r\n",
        "  u6 = tf.keras.layers.concatenate([u6, c4])\r\n",
        "  c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\r\n",
        "  c6 = tf.keras.layers.Dropout(0.2)(c6)\r\n",
        "  c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\r\n",
        "  \r\n",
        "  u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\r\n",
        "  u7 = tf.keras.layers.concatenate([u7, c3])\r\n",
        "  c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\r\n",
        "  c7 = tf.keras.layers.Dropout(0.2)(c7)\r\n",
        "  c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\r\n",
        "  \r\n",
        "  u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\r\n",
        "  u8 = tf.keras.layers.concatenate([u8, c2])\r\n",
        "  c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\r\n",
        "  c8 = tf.keras.layers.Dropout(0.1)(c8)\r\n",
        "  c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\r\n",
        "  \r\n",
        "  u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\r\n",
        "  u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\r\n",
        "  c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\r\n",
        "  c9 = tf.keras.layers.Dropout(0.1)(c9)\r\n",
        "  c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\r\n",
        "  \r\n",
        "  outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\r\n",
        "  \r\n",
        "  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\r\n",
        "  model.compile(optimizer='adam', loss=dice_coef_loss, metrics=['accuracy',dice_coef,iou,])\r\n",
        "  model.summary()\r\n",
        "  return model"
      ],
      "outputs": [],
      "metadata": {
        "id": "lZXmtXtr9YOt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def Unet_2():\r\n",
        "  inputs = Input(shape=(224, 224, 1), name='input')\r\n",
        "  #s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\r\n",
        "\r\n",
        "  #Contraction path\r\n",
        "  c1 = tf.keras.layers.Conv2D(112, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\r\n",
        "  c1 = tf.keras.layers.Dropout(0.1)(c1)\r\n",
        "  c1 = tf.keras.layers.Conv2D(112, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\r\n",
        "  p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\r\n",
        "\r\n",
        "  c2 = tf.keras.layers.Conv2D(224, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\r\n",
        "  c2 = tf.keras.layers.Dropout(0.1)(c2)\r\n",
        "  c2 = tf.keras.layers.Conv2D(224, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\r\n",
        "  p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\r\n",
        "  \r\n",
        "  c3 = tf.keras.layers.Conv2D(448, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\r\n",
        "  c3 = tf.keras.layers.Dropout(0.2)(c3)\r\n",
        "  c3 = tf.keras.layers.Conv2D(448, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\r\n",
        "  p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\r\n",
        "  \r\n",
        "  c4 = tf.keras.layers.Conv2D(448, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\r\n",
        "  c4 = tf.keras.layers.Dropout(0.2)(c4)\r\n",
        "  c4 = tf.keras.layers.Conv2D(448, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\r\n",
        "  \r\n",
        "\r\n",
        "  #Expansive path \r\n",
        "  u6 = tf.keras.layers.Conv2DTranspose(224, (2, 2), strides=(2, 2), padding='same')(c4)\r\n",
        "  u6 = tf.keras.layers.concatenate([u6, c3])\r\n",
        "  c6 = tf.keras.layers.Conv2D(224, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\r\n",
        "  c6 = tf.keras.layers.Dropout(0.2)(c6)\r\n",
        "  c6 = tf.keras.layers.Conv2D(224, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\r\n",
        "  \r\n",
        "  u7 = tf.keras.layers.Conv2DTranspose(112, (2, 2), strides=(2, 2), padding='same')(c6)\r\n",
        "  u7 = tf.keras.layers.concatenate([u7, c2])\r\n",
        "  c7 = tf.keras.layers.Conv2D(112, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\r\n",
        "  c7 = tf.keras.layers.Dropout(0.2)(c7)\r\n",
        "  c7 = tf.keras.layers.Conv2D(112, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\r\n",
        "  \r\n",
        "  u8 = tf.keras.layers.Conv2DTranspose(112, (2, 2), strides=(2, 2), padding='same')(c7)\r\n",
        "  u8 = tf.keras.layers.concatenate([u8, c1])\r\n",
        "  c8 = tf.keras.layers.Conv2D(112, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\r\n",
        "  c8 = tf.keras.layers.Dropout(0.1)(c8)\r\n",
        "  c8 = tf.keras.layers.Conv2D(112, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\r\n",
        "  \r\n",
        "  outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c8)\r\n",
        "  \r\n",
        "  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\r\n",
        "  model.compile(optimizer='adam', loss=dice_coef_loss, metrics=['accuracy',dice_coef,iou,])\r\n",
        "  model.summary()\r\n",
        "  return model"
      ],
      "outputs": [],
      "metadata": {
        "id": "So8yF1HoHQo9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def Unet_3():\r\n",
        "  inputs = Input(shape=(224, 224, 1), name='input')\r\n",
        "  #s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\r\n",
        "\r\n",
        "  #Contraction path\r\n",
        "  c1 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\r\n",
        "  c1 = tf.keras.layers.Dropout(0.1)(c1)\r\n",
        "  c1 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\r\n",
        "  p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\r\n",
        "\r\n",
        "  c2 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\r\n",
        "  c2 = tf.keras.layers.Dropout(0.1)(c2)\r\n",
        "  c2 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\r\n",
        "  p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\r\n",
        "  \r\n",
        "  c3 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\r\n",
        "  c3 = tf.keras.layers.Dropout(0.2)(c3)\r\n",
        "  c3 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\r\n",
        "  p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\r\n",
        "  \r\n",
        "  c4 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\r\n",
        "  c4 = tf.keras.layers.Dropout(0.2)(c4)\r\n",
        "  c4 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\r\n",
        "  p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\r\n",
        "  \r\n",
        "  c5 = tf.keras.layers.Conv2D(1024, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\r\n",
        "  c5 = tf.keras.layers.Dropout(0.3)(c5)\r\n",
        "  c5 = tf.keras.layers.Conv2D(1024, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\r\n",
        "\r\n",
        "  #Expansive path \r\n",
        "  u6 = tf.keras.layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c5)\r\n",
        "  u6 = tf.keras.layers.concatenate([u6, c4])\r\n",
        "  c6 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\r\n",
        "  c6 = tf.keras.layers.Dropout(0.2)(c6)\r\n",
        "  c6 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\r\n",
        "  \r\n",
        "  u7 = tf.keras.layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)\r\n",
        "  u7 = tf.keras.layers.concatenate([u7, c3])\r\n",
        "  c7 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\r\n",
        "  c7 = tf.keras.layers.Dropout(0.2)(c7)\r\n",
        "  c7 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\r\n",
        "  \r\n",
        "  u8 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)\r\n",
        "  u8 = tf.keras.layers.concatenate([u8, c2])\r\n",
        "  c8 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\r\n",
        "  c8 = tf.keras.layers.Dropout(0.1)(c8)\r\n",
        "  c8 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\r\n",
        "  \r\n",
        "  u9 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)\r\n",
        "  u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\r\n",
        "  c9 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\r\n",
        "  c9 = tf.keras.layers.Dropout(0.1)(c9)\r\n",
        "  c9 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\r\n",
        "  \r\n",
        "  outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\r\n",
        "  \r\n",
        "  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\r\n",
        "  model.compile(optimizer='adam', loss=dice_coef_loss, metrics=['accuracy',dice_coef,iou,])\r\n",
        "  model.summary()\r\n",
        "  return model"
      ],
      "outputs": [],
      "metadata": {
        "id": "Su1M6WGZKIE6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def Vggunet():\r\n",
        "  inputs = Input(shape=(224, 224,1), name='input')\r\n",
        "  #s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\r\n",
        "\r\n",
        "  #Contraction path\r\n",
        "  c1 = tf.keras.layers.Conv2D(64, (3, 3), kernel_initializer='he_normal', padding='same')(inputs)\r\n",
        "  c1 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c1)\r\n",
        "  c1 = tf.keras.layers.ReLU()(c1)\r\n",
        "  c1 = tf.keras.layers.Dropout(0.5)(c1)\r\n",
        "  p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\r\n",
        "\r\n",
        "  c2 = tf.keras.layers.Conv2D(64, (3, 3), kernel_initializer='he_normal', padding='same')(p1)\r\n",
        "  c2 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c2)\r\n",
        "  c2 = tf.keras.layers.ReLU()(c2)\r\n",
        "  c2 = tf.keras.layers.Dropout(0.5)(c2)\r\n",
        "  c2 = tf.keras.layers.Conv2D(128, (3, 3), kernel_initializer='he_normal', padding='same')(c2)\r\n",
        "  c2 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c2)\r\n",
        "  c2 = tf.keras.layers.ReLU()(c2)\r\n",
        "  p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\r\n",
        "  \r\n",
        "\r\n",
        "  c3 = tf.keras.layers.Conv2D(128, (3, 3), kernel_initializer='he_normal', padding='same')(p2)\r\n",
        "  c3 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c3)\r\n",
        "  c3 = tf.keras.layers.ReLU()(c3)\r\n",
        "  c3 = tf.keras.layers.Dropout(0.5)(c3)\r\n",
        "  c3 = tf.keras.layers.Conv2D(256, (3, 3), kernel_initializer='he_normal', padding='same')(c3)\r\n",
        "  c3 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c3)\r\n",
        "  c3 = tf.keras.layers.ReLU()(c3)\r\n",
        "  c3 = tf.keras.layers.Dropout(0.5)(c3)\r\n",
        "  c3 = tf.keras.layers.Conv2D(256, (3, 3), kernel_initializer='he_normal', padding='same')(c3)\r\n",
        "  c3 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c3)\r\n",
        "  c3 = tf.keras.layers.ReLU()(c3)\r\n",
        "  p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\r\n",
        "  \r\n",
        "  c4 = tf.keras.layers.Conv2D(256, (3, 3), kernel_initializer='he_normal', padding='same')(p3)\r\n",
        "  c4 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c4)\r\n",
        "  c4 = tf.keras.layers.ReLU()(c4)\r\n",
        "  c4 = tf.keras.layers.Dropout(0.5)(c4)\r\n",
        "  c4 = tf.keras.layers.Conv2D(512, (3, 3), kernel_initializer='he_normal', padding='same')(c4)\r\n",
        "  c4 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c4)\r\n",
        "  c4 = tf.keras.layers.ReLU()(c4)\r\n",
        "  c4 = tf.keras.layers.Dropout(0.5)(c4)\r\n",
        "  c4 = tf.keras.layers.Conv2D(512, (3, 3), kernel_initializer='he_normal', padding='same')(c4)\r\n",
        "  c4 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c4)\r\n",
        "  c4 = tf.keras.layers.ReLU()(c4)\r\n",
        "  p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\r\n",
        "  \r\n",
        "  c5 = tf.keras.layers.Conv2D(512, (3, 3), kernel_initializer='he_normal', padding='same')(p4)\r\n",
        "  c5 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c5)\r\n",
        "  c5 = tf.keras.layers.ReLU()(c5)\r\n",
        "  c5 = tf.keras.layers.Dropout(0.5)(c5)\r\n",
        "  c5 = tf.keras.layers.Conv2D(512, (3, 3), kernel_initializer='he_normal', padding='same')(c5)\r\n",
        "  c5 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c5)\r\n",
        "  c5 = tf.keras.layers.ReLU()(c5)\r\n",
        "  c5 = tf.keras.layers.Dropout(0.5)(c5)\r\n",
        "  c5 = tf.keras.layers.Conv2D(512, (3, 3), kernel_initializer='he_normal', padding='same')(c5)\r\n",
        "  c5 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c5)\r\n",
        "  c5 = tf.keras.layers.ReLU()(c5)\r\n",
        "  p5 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c5)\r\n",
        "\r\n",
        "  c6 = tf.keras.layers.Conv2D(512, (3, 3), kernel_initializer='he_normal', padding='same')(p5)\r\n",
        "  c6 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c6)\r\n",
        "  c6 = tf.keras.layers.ReLU()(c6)\r\n",
        "  c6 = tf.keras.layers.Dropout(0.5)(c6)\r\n",
        "  c6 = tf.keras.layers.Conv2D(512, (3, 3), kernel_initializer='he_normal', padding='same')(c6)\r\n",
        "  c6 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c6)\r\n",
        "  c6 = tf.keras.layers.ReLU()(c6)\r\n",
        "\r\n",
        "  #Expansive path \r\n",
        "  u6 = tf.keras.layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c6)\r\n",
        "  u6 = tf.keras.layers.concatenate([u6, c5])\r\n",
        "  c7 = tf.keras.layers.Conv2D(512, (3, 3), kernel_initializer='he_normal', padding='same')(u6)\r\n",
        "  c7 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c7)\r\n",
        "  c7 = tf.keras.layers.ReLU()(c7)\r\n",
        "  c7 = tf.keras.layers.Dropout(0.5)(c7)\r\n",
        "  c7 = tf.keras.layers.Conv2D(512, (3, 3), kernel_initializer='he_normal', padding='same')(c7)\r\n",
        "  c7 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c7)\r\n",
        "  c7 = tf.keras.layers.ReLU()(c7)\r\n",
        "\r\n",
        "  u7 = tf.keras.layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c7)\r\n",
        "  u7 = tf.keras.layers.concatenate([u7, c4])\r\n",
        "  c8 = tf.keras.layers.Conv2D(512, (3, 3), kernel_initializer='he_normal', padding='same')(u7)\r\n",
        "  c8 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c8)\r\n",
        "  c8 = tf.keras.layers.ReLU()(c8)\r\n",
        "  c8 = tf.keras.layers.Dropout(0.5)(c8)\r\n",
        "  c8 = tf.keras.layers.Conv2D(512, (3, 3), kernel_initializer='he_normal', padding='same')(c8)\r\n",
        "  c8 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c8)\r\n",
        "  c8 = tf.keras.layers.ReLU()(c8)\r\n",
        "  \r\n",
        "  u8 = tf.keras.layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c8)\r\n",
        "  u8 = tf.keras.layers.concatenate([u8, c3])\r\n",
        "  c9 = tf.keras.layers.Conv2D(256, (3, 3), kernel_initializer='he_normal', padding='same')(u8)\r\n",
        "  c9 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c9)\r\n",
        "  c9 = tf.keras.layers.ReLU()(c9)\r\n",
        "  c9 = tf.keras.layers.Dropout(0.5)(c9)\r\n",
        "  c9 = tf.keras.layers.Conv2D(256, (3, 3), kernel_initializer='he_normal', padding='same')(c9)\r\n",
        "  c9 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c9)\r\n",
        "  c9 = tf.keras.layers.ReLU()(c9)\r\n",
        "  \r\n",
        "  u9 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c9)\r\n",
        "  u9 = tf.keras.layers.concatenate([u9, c2])\r\n",
        "  c10 = tf.keras.layers.Conv2D(128, (3, 3), kernel_initializer='he_normal', padding='same')(u9)\r\n",
        "  c10 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c10)\r\n",
        "  c10 = tf.keras.layers.ReLU()(c10)\r\n",
        "  c10 = tf.keras.layers.Dropout(0.5)(c10)\r\n",
        "  c10 = tf.keras.layers.Conv2D(128, (3, 3), kernel_initializer='he_normal', padding='same')(c10)\r\n",
        "  c10 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c10)\r\n",
        "  c10 = tf.keras.layers.ReLU()(c10)\r\n",
        "  \r\n",
        "  u10 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c10)\r\n",
        "  u10 = tf.keras.layers.concatenate([u10, c1])\r\n",
        "  c11 = tf.keras.layers.Conv2D(64, (3, 3), kernel_initializer='he_normal', padding='same')(u10)\r\n",
        "  c11 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c11)\r\n",
        "  c11 = tf.keras.layers.ReLU()(c11)\r\n",
        "  c11 = tf.keras.layers.Dropout(0.5)(c11)\r\n",
        "  c11 = tf.keras.layers.Conv2D(64, (3, 3), kernel_initializer='he_normal', padding='same')(c11)\r\n",
        "  c11 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c11)\r\n",
        "  c11 = tf.keras.layers.ReLU()(c11)\r\n",
        "  \r\n",
        "  outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c11)\r\n",
        "  \r\n",
        "  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\r\n",
        "  model.compile(optimizer='adam', loss=dice_coef_loss, metrics=['accuracy',dice_coef,iou,])\r\n",
        "  model.summary()\r\n",
        "  return model"
      ],
      "outputs": [],
      "metadata": {
        "id": "EqNEEOiWJ-Av"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def Pspnet():\r\n",
        "  inputs = Input(shape=(224, 224,1), name='input')\r\n",
        "  #s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\r\n",
        "\r\n",
        "  #Encoder path\r\n",
        "  c1 = tf.keras.layers.Conv2D(32, (3, 3), kernel_initializer='he_normal', padding='same',dilation_rate=(1, 1))(inputs)\r\n",
        "  c1 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c1)\r\n",
        "  c1 = tf.keras.layers.LeakyReLU(alpha = 0.2)(c1)\r\n",
        "  c1 = tf.keras.layers.Conv2D(32, (3, 3), kernel_initializer='he_normal', padding='same',dilation_rate=(2, 2))(c1)\r\n",
        "  c1 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c1)\r\n",
        "  c1 = tf.keras.layers.LeakyReLU(alpha = 0.2)(c1)\r\n",
        "  c1 = tf.keras.layers.Conv2D(64, (3, 3), kernel_initializer='he_normal', padding='same',dilation_rate=(1, 1))(c1)\r\n",
        "  c1 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c1)\r\n",
        "  cs1 = tf.keras.layers.Conv2D(64, (3, 3), kernel_initializer='he_normal', padding='same')(inputs)\r\n",
        "  cs1 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(cs1)\r\n",
        "  c1 = tf.keras.layers.Add()([c1,cs1])\r\n",
        "  c1 = tf.keras.layers.ReLU()(c1)\r\n",
        "  \r\n",
        "  c2 = tf.keras.layers.Conv2D(64, (3, 3), kernel_initializer='he_normal', padding='same',dilation_rate=(1, 1))(c1)\r\n",
        "  c2 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c2)\r\n",
        "  c2 = tf.keras.layers.LeakyReLU(alpha = 0.2)(c2)\r\n",
        "  c2 = tf.keras.layers.Conv2D(64, (3, 3), kernel_initializer='he_normal', padding='same',dilation_rate=(2, 2))(c2)\r\n",
        "  c2 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c2)\r\n",
        "  c2 = tf.keras.layers.LeakyReLU(alpha = 0.2)(c2)\r\n",
        "  c2 = tf.keras.layers.Conv2D(128, (3, 3), kernel_initializer='he_normal', padding='same',dilation_rate=(1, 1))(c2)\r\n",
        "  c2 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c2)\r\n",
        "  cs2 = tf.keras.layers.Conv2D(128, (3, 3), kernel_initializer='he_normal', padding='same')(c1)\r\n",
        "  cs2 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(cs2)\r\n",
        "  c2 = tf.keras.layers.Add()([c2,cs2])\r\n",
        "  c2 = tf.keras.layers.ReLU()(c2)\r\n",
        "\r\n",
        "  c3 = tf.keras.layers.Conv2D(128, (3, 3), kernel_initializer='he_normal', padding='same',dilation_rate=(1, 1))(c2)\r\n",
        "  c3 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c3)\r\n",
        "  c3 = tf.keras.layers.LeakyReLU(alpha = 0.2)(c3)\r\n",
        "  c3 = tf.keras.layers.Conv2D(128, (3, 3), kernel_initializer='he_normal', padding='same',dilation_rate=(2, 2))(c3)\r\n",
        "  c3 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c3)\r\n",
        "  c3 = tf.keras.layers.LeakyReLU(alpha = 0.2)(c3)\r\n",
        "  c3 = tf.keras.layers.Conv2D(256, (3, 3), kernel_initializer='he_normal', padding='same',dilation_rate=(1, 1))(c3)\r\n",
        "  c3 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c3)\r\n",
        "  cs3 = tf.keras.layers.Conv2D(256, (3, 3), kernel_initializer='he_normal', padding='same')(c2)\r\n",
        "  cs3 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(cs3)\r\n",
        "  c3 = tf.keras.layers.Add()([c3,cs3])\r\n",
        "  c3 = tf.keras.layers.ReLU()(c3)\r\n",
        "\r\n",
        "  #Piramid Feature Map\r\n",
        "  r = tf.keras.layers.GlobalAveragePooling2D()(c3) \r\n",
        "  r = tf.keras.layers.Reshape((1,1,256))(r)\r\n",
        "  r = tf.keras.layers.Conv2D(64, (1, 1))(r)\r\n",
        "  r = tf.keras.layers.UpSampling2D(224,interpolation='bilinear')(r)\r\n",
        "  \r\n",
        "  y = tf.keras.layers.AveragePooling2D((2,2))(c3)\r\n",
        "  y = tf.keras.layers.Conv2D(64,(1,1))(y)\r\n",
        "  y = tf.keras.layers.UpSampling2D(2,interpolation='bilinear')(y)\r\n",
        "  \r\n",
        "  b = tf.keras.layers.AveragePooling2D((4,4))(c3)\r\n",
        "  b = tf.keras.layers.Conv2D(64,(1,1))(b)\r\n",
        "  b = tf.keras.layers.UpSampling2D(4,interpolation='bilinear')(b)\r\n",
        "    \r\n",
        "  g = tf.keras.layers.AveragePooling2D((8,8))(c3)\r\n",
        "  g = tf.keras.layers.Conv2D(64,(1,1))(g)\r\n",
        "  g = tf.keras.layers.UpSampling2D(8,interpolation='bilinear')(g)\r\n",
        "  \r\n",
        "  P = tf.keras.layers.concatenate([c3,r,y,b,g])\r\n",
        "  #P = tf.keras.layers.Add()([c3,P])\r\n",
        "\r\n",
        "  P = tf.keras.layers.Conv2D(256, (3, 3), kernel_initializer='he_normal', padding='same',dilation_rate=(1, 1))(P)\r\n",
        "  P = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(P)\r\n",
        "  P = tf.keras.layers.LeakyReLU(alpha = 0.2)(P)\r\n",
        "  P = tf.keras.layers.Conv2D(128, (3, 3), kernel_initializer='he_normal', padding='same',dilation_rate=(1, 1))(P)\r\n",
        "  P = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(P)\r\n",
        "  P = tf.keras.layers.LeakyReLU(alpha = 0.2)(P)\r\n",
        "  outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(P)\r\n",
        "  \r\n",
        "  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\r\n",
        "  model.compile(optimizer='adam', loss=dice_coef_loss, metrics=['accuracy',dice_coef,iou,])\r\n",
        "  model.summary()\r\n",
        "  return model"
      ],
      "outputs": [],
      "metadata": {
        "id": "ty9s7aHVjJ05"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def Segnet1():\r\n",
        "  inputs = Input(shape=(224, 224,1), name='input')\r\n",
        "  #s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\r\n",
        "\r\n",
        "  #Encoder path\r\n",
        "  c1 = tf.keras.layers.Conv2D(64, (3, 3), kernel_initializer='he_normal', padding='same')(inputs)\r\n",
        "  c1 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c1)\r\n",
        "  c1 = tf.keras.layers.ReLU()(c1)\r\n",
        "  p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\r\n",
        "\r\n",
        "  c2 = tf.keras.layers.Conv2D(128, (3, 3), kernel_initializer='he_normal', padding='same')(p1)\r\n",
        "  c2 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c2)\r\n",
        "  c2 = tf.keras.layers.ReLU()(c2)\r\n",
        "  p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\r\n",
        "  \r\n",
        "\r\n",
        "  c3 = tf.keras.layers.Conv2D(256, (3, 3), kernel_initializer='he_normal', padding='same')(p2)\r\n",
        "  c3 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c3)\r\n",
        "  c3 = tf.keras.layers.ReLU()(c3)\r\n",
        "  p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\r\n",
        "  \r\n",
        "  c4 = tf.keras.layers.Conv2D(512, (3, 3), kernel_initializer='he_normal', padding='same')(p3)\r\n",
        "  c4 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c4)\r\n",
        "  c4 = tf.keras.layers.ReLU()(c4)\r\n",
        "  p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\r\n",
        "\r\n",
        "  #Decoder path \r\n",
        "  c5 = tf.keras.layers.Conv2D(512, (3, 3), kernel_initializer='he_normal', padding='same')(p4)\r\n",
        "  c5 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c5)\r\n",
        "  c5 = tf.keras.layers.ReLU()(c5)\r\n",
        "  u1 = tf.keras.layers.UpSampling2D(size = (2, 2))(c5)\r\n",
        "\r\n",
        "  \r\n",
        "  c6 = tf.keras.layers.Conv2D(256, (3, 3), kernel_initializer='he_normal', padding='same')(u1)\r\n",
        "  c6 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c6)\r\n",
        "  c6 = tf.keras.layers.ReLU()(c6)\r\n",
        "  u2 = tf.keras.layers.UpSampling2D(size = (2, 2))(c6)\r\n",
        "  \r\n",
        "  c7 = tf.keras.layers.Conv2D(128, (3, 3), kernel_initializer='he_normal', padding='same')(u2)\r\n",
        "  c7 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c7)\r\n",
        "  c7 = tf.keras.layers.ReLU()(c7)\r\n",
        "  u3 = tf.keras.layers.UpSampling2D(size = (2, 2))(c7)\r\n",
        "  \r\n",
        "  c8 = tf.keras.layers.Conv2D(64, (3, 3), kernel_initializer='he_normal', padding='same')(u3)\r\n",
        "  c8 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(c8)\r\n",
        "  c8 = tf.keras.layers.ReLU()(c8)\r\n",
        "  u4 = tf.keras.layers.UpSampling2D(size = (2, 2))(c8)\r\n",
        "  \r\n",
        "  outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(u4)\r\n",
        "  \r\n",
        "  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\r\n",
        "  model.compile(optimizer='adam', loss=dice_coef_loss, metrics=['accuracy',dice_coef,iou,])\r\n",
        "  model.summary()\r\n",
        "  return model"
      ],
      "outputs": [],
      "metadata": {
        "id": "-1RzuFeKqeIr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def VGGUnet_pretrained():\r\n",
        "    model = tf.keras.applications.VGG16(weights='imagenet', include_top=False)\r\n",
        "\r\n",
        "    # Block1_conv1 weights are of the format [3, 3, 3, 64] -> this is for RGB images\r\n",
        "    # For grayscale, format should be [3, 3, 1, 64]. Weighted average of the features has to be calculated across channels.\r\n",
        "    # RGB weights: Red 0.2989, Green 0.5870, Blue 0.1140\r\n",
        "\r\n",
        "    # getting weights of block1 conv1.\r\n",
        "    block1_conv1 = model.get_layer('block1_conv1').get_weights()\r\n",
        "    weights, biases = block1_conv1\r\n",
        "\r\n",
        "    # :weights shape = [3, 3, 3, 64] - (0, 1, 2, 3)\r\n",
        "    # convert :weights shape to = [64, 3, 3, 3] - (3, 2, 0, 1)\r\n",
        "    weights = np.transpose(weights, (3, 2, 0, 1))\r\n",
        "\r\n",
        "\r\n",
        "    kernel_out_channels, kernel_in_channels, kernel_rows, kernel_columns = weights.shape\r\n",
        "\r\n",
        "    # Dimensions : [kernel_out_channels, 1 (since grayscale), kernel_rows, kernel_columns]\r\n",
        "    grayscale_weights = np.zeros((kernel_out_channels, 1, kernel_rows, kernel_columns))\r\n",
        "\r\n",
        "      # iterate out_channels number of times\r\n",
        "    for i in range(kernel_out_channels):\r\n",
        "\r\n",
        "        # get kernel for every out_channel\r\n",
        "        get_kernel = weights[i, :, :, :]\r\n",
        "\r\n",
        "        temp_kernel = np.zeros((3, 3))\r\n",
        "\r\n",
        "        # :get_kernel shape = [3, 3, 3]\r\n",
        "        # axis, dims = (0, in_channel), (1, row), (2, col)\r\n",
        "\r\n",
        "        # calculate weighted average across channel axis\r\n",
        "        in_channels, in_rows, in_columns = get_kernel.shape\r\n",
        "\r\n",
        "        for in_row in range(in_rows):\r\n",
        "            for in_col in range(in_columns):\r\n",
        "                feature_red = get_kernel[0, in_row, in_col]\r\n",
        "                feature_green = get_kernel[1, in_row, in_col]\r\n",
        "                feature_blue = get_kernel[2, in_row, in_col]\r\n",
        "\r\n",
        "            # weighted average for RGB filter\r\n",
        "            total = (feature_red * 0.2989) + (feature_green * 0.5870) + (feature_blue * 0.1140)\r\n",
        "\r\n",
        "            temp_kernel[in_row, in_col] = total\r\n",
        "\r\n",
        "\r\n",
        "        # :temp_kernel is a 3x3 matrix [rows x columns]\r\n",
        "        # add an axis at the end to specify in_channel as 1\r\n",
        "\r\n",
        "        # Second: Add axis at the start of :temp_kernel to make its shape: [1, 3, 3] which is [in_channel, rows, columns]\r\n",
        "        temp_kernel = np.expand_dims(temp_kernel, axis=0)\r\n",
        "\r\n",
        "        # Now, :temp_kernel shape is [1, 3, 3]\r\n",
        "\r\n",
        "        # Concat :temp_kernel to :grayscale_weights along axis=0\r\n",
        "        grayscale_weights[i, :, :, :] = temp_kernel\r\n",
        "\r\n",
        "      # Dimension of :grayscale_weights is [64, 1, 3, 3]\r\n",
        "      # In order to bring it to tensorflow or keras weight format, transpose :grayscale_weights\r\n",
        "\r\n",
        "      # dimension, axis of :grayscale_weights = (out_channels: 0), (in_channels: 1), (rows: 2), (columns: 3)\r\n",
        "      # tf format of weights = (rows: 0), (columns: 1), (in_channels: 2), (out_channels: 3)\r\n",
        "\r\n",
        "      # Go from (0, 1, 2, 3) to (2, 3, 1, 0)\r\n",
        "    grayscale_weights = np.transpose(grayscale_weights, (2, 3, 1, 0)) # (3, 3, 1, 64)\r\n",
        "\r\n",
        "      # combine :grayscale_weights and :biases\r\n",
        "    new_block1_conv1 = [grayscale_weights, biases]\r\n",
        "\r\n",
        "\r\n",
        "      # Reconstruct the layers of VGG16 but replace block1_conv1 weights with :grayscale_weights\r\n",
        "\r\n",
        "      # get weights of all the layers starting from 'block1_conv2'\r\n",
        "    vgg16_weights = {}\r\n",
        "    for layer in model.layers[2:]:\r\n",
        "        if \"conv\" in layer.name:\r\n",
        "            vgg16_weights[\"224_\" + layer.name] = model.get_layer(layer.name).get_weights()\r\n",
        "\r\n",
        "    del model\r\n",
        "\r\n",
        "\r\n",
        "      # Custom build VGG16\r\n",
        "    input = Input(shape=(224, 224, 1), name='224_input')\r\n",
        "      # Block 1\r\n",
        "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(224, 224, 1), data_format=\"channels_last\", name='224_block1_conv1')(input)\r\n",
        "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same', name='224_block1_conv2')(c1)\r\n",
        "    p1 = MaxPooling2D((2, 2), strides=(2, 2), name='224_block1_pool')(c1)\r\n",
        "\r\n",
        "      # Block 2\r\n",
        "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same', name='224_block2_conv1')(p1)\r\n",
        "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same', name='224_block2_conv2')(c2)\r\n",
        "    p2 = MaxPooling2D((2, 2), strides=(2, 2), name='224_block2_pool')(c2)\r\n",
        "\r\n",
        "      # Block 3\r\n",
        "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same', name='224_block3_conv1')(p2)\r\n",
        "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same', name='224_block3_conv2')(c3)\r\n",
        "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same', name='224_block3_conv3')(c3)\r\n",
        "    p3 = MaxPooling2D((2, 2), strides=(2, 2), name='224_block3_pool')(c3)\r\n",
        "\r\n",
        "      # Block 4\r\n",
        "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same', name='224_block4_conv1')(p3)\r\n",
        "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same', name='224_block4_conv2')(c4)\r\n",
        "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same', name='224_block4_conv3')(c4)\r\n",
        "    p4 = MaxPooling2D((2, 2), strides=(2, 2), name='224_block4_pool')(c4)\r\n",
        "\r\n",
        "      # Block 5\r\n",
        "    c5 = Conv2D(512, (3, 3), activation='relu', padding='same', name='224_block5_conv1')(p4)\r\n",
        "    c5 = Conv2D(512, (3, 3), activation='relu', padding='same', name='224_block5_conv2')(c5)\r\n",
        "    c5 = Conv2D(512, (3, 3), activation='relu', padding='same', name='224_block5_conv3')(c5)\r\n",
        "    p5 = MaxPooling2D((2, 2), strides=(2, 2), name='224_block5_pool')(c5)\r\n",
        "\r\n",
        "    base_model = Model(inputs=input, outputs=p5)\r\n",
        "\r\n",
        "    base_model.get_layer('224_block1_conv1').set_weights(new_block1_conv1)\r\n",
        "    for layer in base_model.layers[2:]:\r\n",
        "        if 'conv' in layer.name:\r\n",
        "            base_model.get_layer(layer.name).set_weights(vgg16_weights[layer.name])\r\n",
        "\r\n",
        "    x = base_model.output\r\n",
        "\r\n",
        "    for layer in base_model.layers:\r\n",
        "        layer.trainable = True\r\n",
        "\r\n",
        "    u6 = tf.keras.layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same', name='224_block6_upsam')(x)\r\n",
        "    u6 = tf.keras.layers.concatenate([u6, c5], name='224_block6_concat')\r\n",
        "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same', name='224_block6_conv1')(u6)\r\n",
        "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same', name='224_block6_conv2')(c6)\r\n",
        "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same', name='224_block6_conv3')(c6)\r\n",
        "  \r\n",
        "    u7 = tf.keras.layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same', name='224_block7_upsam')(c6)\r\n",
        "    u7 = tf.keras.layers.concatenate([u7, c4], name='224_block7_concat')\r\n",
        "    c7 = Conv2D(512, (3, 3), activation='relu', padding='same', name='224_block7_conv1')(u7)\r\n",
        "    c7 = Conv2D(512, (3, 3), activation='relu', padding='same', name='224_block7_conv2')(c7)\r\n",
        "    c7 = Conv2D(512, (3, 3), activation='relu', padding='same', name='224_block7_conv3')(c7)\r\n",
        "\r\n",
        "    u8 = tf.keras.layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same', name='224_block8_upsam')(c7)\r\n",
        "    u8 = tf.keras.layers.concatenate([u8, c3], name='224_block8_concat')\r\n",
        "    c8 = Conv2D(256, (3, 3), activation='relu', padding='same', name='224_block8_conv1')(u8)\r\n",
        "    c8 = Conv2D(256, (3, 3), activation='relu', padding='same', name='224_block8_conv2')(c8)\r\n",
        "    c8 = Conv2D(256, (3, 3), activation='relu', padding='same', name='224_block8_conv3')(c8)\r\n",
        "\r\n",
        "    u9 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same', name='224_block9_upsam')(c8)\r\n",
        "    u9 = tf.keras.layers.concatenate([u9, c2], name='224_block9_concat')\r\n",
        "    c9 = Conv2D(128, (3, 3), activation='relu', padding='same', name='224_block9_conv1')(u9)\r\n",
        "    c9 = Conv2D(128, (3, 3), activation='relu', padding='same', name='224_block9_conv2')(c9)\r\n",
        "\r\n",
        "    u10 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same', name='224_block10_upsam')(c9)\r\n",
        "    u10 = tf.keras.layers.concatenate([u10, c1], name='224_block10_concat')\r\n",
        "    c10 = Conv2D(64, (3, 3), activation='relu', padding='same', name='224_block10_conv1')(u10)\r\n",
        "    c10 = Conv2D(64, (3, 3), activation='relu', padding='same', name='224_block10_conv2')(c10)\r\n",
        "      \r\n",
        "    predictions = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c10)\r\n",
        "    #Compilador\r\n",
        "    model = tf.keras.Model(inputs=[base_model.input], outputs=[predictions])\r\n",
        "    model.compile(optimizer='adam', loss=dice_coef_loss, metrics=['accuracy',dice_coef,iou,])\r\n",
        "    model.summary()\r\n",
        "    return model"
      ],
      "outputs": [],
      "metadata": {
        "id": "PaCb-F1xwEvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "yX8zz1PX-Z9O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model = Unet_3()"
      ],
      "outputs": [],
      "metadata": {
        "id": "HuulAYYJ6-nk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ca2b6bf-2699-4a39-94e0-c03c4686b85a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\"\"\"\r\n",
        "model_name='VggUnet_pretrained_M+J+N_1'\r\n",
        "log_dir=\"logs/\"\r\n",
        "filepath = log_dir+\"Saved_models/\"+model_name+\".h5\"\r\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', save_best_only=True, mode='max')\r\n",
        "csv_logger = CSVLogger('logs/csv/'+model_name+'.csv', append=False, separator=';')\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "history=model.fit(X_train, Y_train, epochs=200, batch_size=32, validation_data=(X_dev, Y_dev))#,callbacks=[csv_logger,checkpoint])"
      ],
      "outputs": [],
      "metadata": {
        "id": "gyvKJusX60BT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b806f70d-ec3f-4ac0-9fba-e2b701b0d412"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "plt.figure()\r\n",
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.title('model loss')\r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.ylabel('Loss')\r\n",
        "plt.legend(['train', 'validation'], loc='upper left')"
      ],
      "outputs": [],
      "metadata": {
        "id": "t76dF06b-8Uw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "07cf9921-e1e6-4d57-eb90-43e6d4c3244b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "plt.figure()\r\n",
        "plt.plot(history.history['dice_coef'])\r\n",
        "plt.plot(history.history['val_dice_coef'])\r\n",
        "plt.title('model Accuracy')\r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.ylabel('Accuracy')\r\n",
        "plt.legend(['train', 'validation'], loc='upper left')"
      ],
      "outputs": [],
      "metadata": {
        "id": "TvSZ8RT6_tR9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "d558dbd7-caba-418d-c8ec-0ffe1d74e621"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model.save('logs/Saved_models/Segmentation_model')"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5WKI5q1lr86",
        "outputId": "6f3ccd72-4d8e-466c-843e-c7fd82091f31"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model=tf.keras.models.load_model('logs/Saved_models/Segmentation_model',compile=False,custom_objects={'iou':iou,'dice_coef':dice_coef})"
      ],
      "outputs": [],
      "metadata": {
        "id": "uDXg0YtiItm6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model.summary()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYYc_xdWOEoY",
        "outputId": "9e37ef64-492b-4eac-c33d-9a40490aecd8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "g=pd.read_csv('logs/csv/'+model_name+'.csv', sep=';')\r\n",
        "for i in range(len(g)):\r\n",
        "  if g['val_dice_coef'][i]==np.max(g['val_dice_coef']):\r\n",
        "    print('dice=',g['dice_coef'][i])\r\n",
        "    print('Iou=',g['iou'][i])\r\n",
        "    print('val dice=',g['val_dice_coef'][i])\r\n",
        "    print('val_Iou=',g['val_iou'][i])"
      ],
      "outputs": [],
      "metadata": {
        "id": "FP-STgiX2Ne1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbd2b6b1-f8b2-499a-da4d-048c45d653f7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Metrics"
      ],
      "metadata": {
        "id": "Pkr9cZi4AAdS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model.load_weights('logs/Saved_models/Unet_3_M+J+N_1.h5')"
      ],
      "outputs": [],
      "metadata": {
        "id": "P__B_rhmmWTP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X_C_Train = np.load('X_Covid_Norm_2.npy')\r\n",
        "#X_C_Dev = np.load('X_chesxpert_dev.npy')\r\n",
        "#X_C_Test = np.load('X_chesxpert_test.npy')"
      ],
      "outputs": [],
      "metadata": {
        "id": "KkIoU4Swrqnm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "87e066cf-343a-4538-c9f2-b046db18ffd1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X_C_Train_2=min_max_preprocessing(X_C_Train)\r\n",
        "#X_C_Train_2=featurewise_preprocessing(X_C_Train_2,mean,std)"
      ],
      "outputs": [],
      "metadata": {
        "id": "O9_a6dlUvB3u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "P_C_Train = model.predict(X_C_Train_2)\r\n",
        "pred_img=[]\r\n",
        "for i in P_C_Train:\r\n",
        "  pred_img.append(np.resize(i,(224,224)))"
      ],
      "outputs": [],
      "metadata": {
        "id": "pXH94F0wrl3h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "np.unique(P_C_Train)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOzk0K5VQ7jh",
        "outputId": "f2089449-ed8a-40b8-a4ec-774f0375c06d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "thresh= 0.3\r\n",
        "pred = []\r\n",
        "for a in pred_img:\r\n",
        "  a[a >= thresh] = 1\r\n",
        "  a[a < thresh] = 0\r\n",
        "  a = np.logical_not(a)\r\n",
        "  pred.append(a)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Jgy7ZPqMRHql"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "np.unique(pred)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZALxMDP9RQLN",
        "outputId": "5b130cc1-4e8f-4790-a7c6-a53234916242"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X_CS_Train = pred*X_C_Train"
      ],
      "outputs": [],
      "metadata": {
        "id": "fG5WIlUSSP1U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "np.save('X_Covid_Cohen_C_Seg_I.npy',X_CS_Train)"
      ],
      "outputs": [],
      "metadata": {
        "id": "N_W0tHf5Sxyl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "plt.figure(1)\r\n",
        "plt.imshow(X_C_Train[0],cmap='gray')\r\n",
        "plt.axis('off')\r\n",
        "plt.figure(2)\r\n",
        "plt.imshow(pred[0],cmap='gray')\r\n",
        "plt.axis('off')\r\n",
        "plt.figure(3)\r\n",
        "plt.imshow(X_CS_Train[0],cmap='gray')\r\n",
        "plt.axis('off')"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        },
        "id": "kIULrMsYPpDo",
        "outputId": "847075c3-b22c-423a-c204-ed180a1d62ff"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X_P_Train = np.load('X_pos_train_C.npy')\r\n",
        "X_P_Dev = np.load('X_pos_dev_C.npy')\r\n",
        "X_P_Test = np.load('X_pos_test_C.npy')"
      ],
      "outputs": [],
      "metadata": {
        "id": "HwZgejtDsfMO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "P_P_Train = model.predict(X_P_Train)\r\n",
        "P_P_Dev = model.predict(X_P_Dev)\r\n",
        "P_P_Test = model.predict(X_P_Test)"
      ],
      "outputs": [],
      "metadata": {
        "id": "b_2ami0js0_j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X_CS_Train = P_C_Train[:,:,:,0]*X_C_Train"
      ],
      "outputs": [],
      "metadata": {
        "id": "kNYjMw3Qs51w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "Prueba = model.predict(X_dev)\r\n",
        "pred_img=[]\r\n",
        "for i in Prueba:\r\n",
        "  pred_img.append(np.resize(i,(224,224)))"
      ],
      "outputs": [],
      "metadata": {
        "id": "rgbsjAZcqr10"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "plt.figure(1)\r\n",
        "plt.imshow(X_dev[1],cmap='gray')\r\n",
        "plt.axis('off')\r\n",
        "plt.figure(2)\r\n",
        "plt.imshow(Y_dev[1],cmap='gray')\r\n",
        "plt.axis('off')\r\n",
        "plt.figure(3)\r\n",
        "plt.imshow(pred_img[1],cmap='gray')\r\n",
        "plt.axis('off')"
      ],
      "outputs": [],
      "metadata": {
        "id": "gbCaJE9ctbDI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "plt.figure(1)\r\n",
        "plt.imshow(X_dev[2],cmap='gray')\r\n",
        "plt.axis('off')\r\n",
        "plt.figure(2)\r\n",
        "plt.imshow(Y_dev[2],cmap='gray')\r\n",
        "plt.axis('off')\r\n",
        "plt.figure(3)\r\n",
        "plt.imshow(pred_img[2],cmap='gray')\r\n",
        "plt.axis('off')"
      ],
      "outputs": [],
      "metadata": {
        "id": "PJ5pSIKnkzPk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X_test_C = np.load('X_pos_test_C.npy')"
      ],
      "outputs": [],
      "metadata": {
        "id": "tTyzQbY1drQY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X_test_C=min_max_preprocessing(X_test_C)\r\n",
        "X_test_C=featurewise_preprocessing(X_test_C,mean,std)"
      ],
      "outputs": [],
      "metadata": {
        "id": "X_dMmp9fJxUN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "predict=model.predict(np.expand_dims(X_test_C[0],axis=0))"
      ],
      "outputs": [],
      "metadata": {
        "id": "pq9bFh6ZPeIO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "f=plt.figure()\r\n",
        "plt.subplot(1,3,1)\r\n",
        "plt.imshow(X_test_C[2],cmap='gray')\r\n",
        "plt.title('Image')\r\n",
        "plt.axis('off')\r\n",
        "plt.subplot(1,3,2)\r\n",
        "plt.imshow(pred_img_1[2],cmap='gray')\r\n",
        "plt.title('Unet 1 result')\r\n",
        "plt.axis('off')\r\n",
        "plt.subplot(1,3,3)\r\n",
        "plt.imshow(pred_img_2[2],cmap='gray')\r\n",
        "plt.title('Unet 3 result')\r\n",
        "plt.axis('off')\r\n",
        "plt.show()\r\n",
        "f.savefig('IMG paper/ImgUnets.pdf', bbox_inches='tight')"
      ],
      "outputs": [],
      "metadata": {
        "id": "P5QwameY-U-z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "0a259e78-8f8f-44d3-e184-e9580b13cfbd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "num_rows = 2\r\n",
        "num_cols = 20\r\n",
        "num_images = 20\r\n",
        "plt.figure(figsize=(num_cols, num_rows))\r\n",
        "for i in tqdm(range(num_images)):\r\n",
        "    plt.subplot(num_rows, num_cols, 2*i+1)\r\n",
        "    plt.imshow(X_test_C[i])\r\n",
        "    plt.title('Image')\r\n",
        "    plt.subplot(num_rows, num_cols, 2*i+2)\r\n",
        "    plt.imshow(pred_img_1[i])\r\n",
        "    plt.title('Mask')\r\n",
        "plt.tight_layout()"
      ],
      "outputs": [],
      "metadata": {
        "id": "11qpjn4ge2Mx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "3895fe24-69fd-40de-83f2-1d1794b90009"
      }
    }
  ]
}