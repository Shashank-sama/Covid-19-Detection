{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D,Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dropout, Activation, Flatten, Concatenate, Dense, Reshape, Add, PReLU, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, Adagrad, SGD, Adadelta\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.applications import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from tensorflow.keras.metrics import *\n",
    "import os\n",
    "from keras.models import load_model\n",
    "import glob, os.path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import os\n",
    "from skimage import io\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pydicom\n",
    "\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nifty2numpy(nifti_path):\n",
    "    img = nib.load(nifti_path)\n",
    "    return np.array(img.dataobj)\n",
    "\n",
    "\n",
    "def dicom2numpy(dicom_path):\n",
    "    dicom_file = pydicom.read_file(dicom_path)\n",
    "    return np.array(dicom_file.pixel_array), dicom_file.PhotometricInterpretation\n",
    "\n",
    "def dicom2numpy_2(dicom_path):\n",
    "    dicom_file = pydicom.read_file(dicom_path)\n",
    "    return np.array(dicom_file.pixel_array), dicom_file.PhotometricInterpretation,dicom_file\n",
    "\n",
    "\n",
    "def json_to_dict(p):\n",
    "    dti = dict()\n",
    "    path = p.replace(PATH, \"\").split(\"/\")\n",
    "    modality = path[-1].replace(\".png\", \"\").split(\"_\")[-1]    \n",
    "    dti['Subject'] = path[0]\n",
    "    dti['Session'] = path[1]\n",
    "    dti['mod'] = path[2]\n",
    "    dti['File'] = path[3]\n",
    "    dti['Type'] = modality\n",
    "    dti['Path'] = p\n",
    "    json_file = p.replace('.png', '.json').replace('.nii.gz', '.json')\n",
    "    # If there is no json associated with the file, then pick the json on the series. \n",
    "    # Maybe multiple acquisitions with the same parameters in the same series?\n",
    "    if not os.path.exists(json_file):\n",
    "        json_in_series = glob.glob(os.path.join(PATH, path[0], path[1], path[2], '*.json'))\n",
    "        if len(json_in_series) >= 1:\n",
    "            json_file = json_in_series[0]\n",
    "    if os.path.exists(json_file):\n",
    "        with open(json_file) as f:\n",
    "            dtij = json.load(f)\n",
    "        for l in dtij.keys():\n",
    "            if 'Value' in dtij[l].keys():\n",
    "                try:\n",
    "                    dti[pydicom.datadict.dictionary_description(str(l))] = dtij[l]['Value']\n",
    "                except:\n",
    "                    dti[str(l)] = dtij[l]['Value']\n",
    "    else:\n",
    "        print(p)\n",
    "    return dti\n",
    "\n",
    "def min_max_preprocessing(images):\n",
    "    processed_images = []\n",
    "    for i in range(len(images)):\n",
    "        maxi=np.max(images[i])\n",
    "        mini=np.min(images[i])\n",
    "        processed_images.append((images[i]-mini)/(maxi-mini))\n",
    "    return np.array(processed_images)\n",
    "\n",
    "def samplewise_preprocessing(images):\n",
    "    processed_images = []\n",
    "    means = []\n",
    "    stds = []\n",
    "    for i in range(images.shape[0]):\n",
    "        mean = np.mean(images[i])\n",
    "        std = np.std(images[i])\n",
    "        if std!=0 and mean != 0:\n",
    "            means.append(mean)\n",
    "            stds.append(std)\n",
    "            processed_images.append((images[i]-mean)/std)\n",
    "    \n",
    "    return np.array(processed_images), np.mean(means), np.mean(stds)\n",
    "\n",
    "\n",
    "def featurewise_preprocessing(images, mean, std):\n",
    "    processed_images = np.zeros_like(images, dtype=np.float32)\n",
    "    for i in range(images.shape[0]):\n",
    "        processed_images[i] = (images[i]-mean)/std\n",
    "    return processed_images\n",
    "\n",
    "def checkDuplicates(trainDF, devDF, testDF,id_column):\n",
    "    patientsTrain = set(trainDF[id_column])\n",
    "    patientsDev = set(devDF[id_column])\n",
    "    patientsTest = set(testDF[id_column])\n",
    "\n",
    "    ids = list(patientsTrain.intersection(patientsDev))\n",
    "    print('# de pacientes de train presentes en dev:', len(ids))\n",
    "\n",
    "    ids_ = list(patientsTrain.intersection(patientsTest))\n",
    "    print('# de pacientes de train presentes en test:', len(ids_))\n",
    "    ids.extend(ids_)\n",
    "\n",
    "    ids_dev = list(patientsDev.intersection(patientsTest))\n",
    "    print('# de pacientes de dev presentes en test:', len(ids_dev))\n",
    "\n",
    "def remove_black_borders_1(img):\n",
    "    rows, cols = img.shape\n",
    "    start_row, end_row = 0, 0\n",
    "    start_col, end_col = 0, 0\n",
    "    \n",
    "    for i in range(rows):\n",
    "        if sum(img[i, :]) > 0:\n",
    "            start_row = i\n",
    "            break\n",
    "    for i in reversed(range(rows)):\n",
    "        if (sum(img[i, :]) > 0):\n",
    "            end_row = i\n",
    "            break    \n",
    "    for i in range(cols):\n",
    "        if (sum(img[:, i]) > 0):\n",
    "            start_col = i\n",
    "            break\n",
    "    for i in reversed(range(cols)):\n",
    "        if (sum(img[:, i]) > 0):\n",
    "            end_col = i\n",
    "            break\n",
    "   \n",
    "    return img[start_row:end_row, start_col:end_col]\n",
    "    \n",
    "\n",
    "def saveNPY_from_DICOM(DF,destination, name,path,src_column,W=224,H=224,C_Labels=False,rmv_black_borders=False):\n",
    "    src_dir = path\n",
    "    images = []\n",
    "\n",
    "    print('reading images...')\n",
    "\n",
    "    for i in tqdm(range(len(DF))):\n",
    "        src_file = os.path.join(src_dir, DF[src_column][i])\n",
    "        img,ph,dicom=dicom2numpy_2(src_file)\n",
    "        #img = apply_voi_lut(dicom.pixel_array, dicom)\n",
    "        if ph=='MONOCHROME1':\n",
    "            img=np.amax(img)-img\n",
    "        \n",
    "        if rmv_black_borders:\n",
    "            try:\n",
    "                \n",
    "                img=remove_black_borders_1(img)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        try:\n",
    "            resized = cv2.resize(img, (W, H))\n",
    "        except:\n",
    "            continue\n",
    "        if resized.shape==(W,H,4):\n",
    "            images.append(resized[:,:,0])\n",
    "        else:\n",
    "            images.append(resized)\n",
    "\n",
    "    NPY = np.array(images)\n",
    "    images_filename = destination+'X_'+name+'.npy'\n",
    "    np.save(images_filename, NPY)\n",
    "    if C_Labels:\n",
    "\n",
    "        labels_ = DF.group.replace(['C', 'N', 'I', 'NI'], [0, 1, 2, 3])\n",
    "        labels = tf.keras.utils.to_categorical(labels_, num_classes=4)\n",
    "  \n",
    "        labels_filename = destination+'/y_'+name+'.npy'\n",
    "        np.save(labels_filename, labels)\n",
    " \n",
    "    \n",
    "\n",
    "    print('done!')\n",
    "    \n",
    "    \n",
    "from keras import backend as K\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "  smooth = 1\n",
    "  intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "  return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "  return 1-dice_coef(y_true, y_pred)\n",
    "\n",
    "def iou(y_true, y_pred):\n",
    "    def f(y_true, y_pred):\n",
    "        intersection = (y_true * y_pred).sum()\n",
    "        smooth = 1\n",
    "        union = y_true.sum() + y_pred.sum() - intersection\n",
    "        x = (intersection + smooth) / (union + smooth)\n",
    "        x = x.astype(np.float32)\n",
    "        return x\n",
    "    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "\n",
    "def focal_loss(y_true, y_pred):\n",
    "    alpha=0.25\n",
    "    gamma=2\n",
    "    def focal_loss_with_logits(logits, targets, alpha, gamma, y_pred):\n",
    "        weight_a = alpha * (1 - y_pred) ** gamma * targets\n",
    "        weight_b = (1 - alpha) * y_pred ** gamma * (1 - targets)\n",
    "        return (tf.math.log1p(tf.exp(-tf.abs(logits))) + tf.nn.relu(-logits)) * (weight_a + weight_b) + logits * weight_b\n",
    "\n",
    "    y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "    logits = tf.math.log(y_pred / (1 - y_pred))\n",
    "    loss = focal_loss_with_logits(logits=logits, targets=y_true, alpha=alpha, gamma=gamma, y_pred=y_pred)\n",
    "    # or reduce_sum and/or axis=-1\n",
    "    return tf.reduce_mean(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names =  ['subject','session','filepath','partition','projection']\n",
    "SES_df  = pd.DataFrame(columns = col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positives\n",
    "PATH=\"/media/ia/DATA/COVID19/Xnat_positivas/\"\n",
    "Subjects=os.listdir(PATH)\n",
    "for sub in tqdm(Subjects):\n",
    "    sessions=os.listdir(PATH+sub)\n",
    "    for sess in sessions:\n",
    "        subsess=os.listdir(PATH+sub+'/'+sess)\n",
    "        for subs in subsess:\n",
    "            folder=os.listdir(PATH+sub+'/'+sess+'/'+subs)\n",
    "            for f in folder:\n",
    "                if f=='DICOM':\n",
    "                    images=os.listdir(PATH+sub+'/'+sess+'/'+subs+'/'+f)\n",
    "                    for img in images:\n",
    "                        list_info=[sub,sess,sub+'/'+sess+'/'+subs+'/'+f+'/'+img,'NR','NR']\n",
    "                        row = pd.Series(list_info, index=SES_df.columns)\n",
    "                        SES_df=SES_df.append(row, ignore_index=True)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Establishing the current work directory (cwd)\n",
    "thisdir = '/media/ia/DATA/COVID19/Negativas XNAT/p0032021/'\n",
    "Files = []\n",
    "names=[]\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(thisdir):\n",
    "    for file in f:\n",
    "        if \".dcm\" in file:\n",
    "            Files.append(os.path.join(r,file))\n",
    "            names.append(file)\n",
    "filenames = Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=[]\n",
    "for i,f in enumerate(filenames):\n",
    "    img,_,dicom=dicom2numpy_2(f)\n",
    "    if dicom.Manufacturer=='GE Healthcare':#Select just GE Healthcare Manufacturer to test models\n",
    "        files.append(names[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Negatives\n",
    "PATH='/media/ia/DATA/COVID19/Negativas XNAT/p0032021/'\n",
    "Subjects=os.listdir(PATH)\n",
    "for sub in tqdm(Subjects):\n",
    "    sessions=os.listdir(PATH+sub)\n",
    "    for sess in sessions:\n",
    "        subsess=os.listdir(PATH+sub+'/'+sess)\n",
    "        for subs in subsess:\n",
    "            folder=os.listdir(PATH+sub+'/'+sess+'/'+subs)\n",
    "            for f in folder:\n",
    "                if f=='DICOM':\n",
    "                    images=os.listdir(PATH+sub+'/'+sess+'/'+subs+'/'+f)\n",
    "                    \n",
    "                    for img in images:\n",
    "                        if img in files:\n",
    "                            list_info=[sub,sess,sub+'/'+sess+'/'+subs+'/'+f+'/'+img,'NR','NR']\n",
    "                            row = pd.Series(list_info, index=SES_df.columns)\n",
    "                            SES_df=SES_df.append(row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SES_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SES_df.to_csv(\"/media/ia/DATA/COVID19/SES_DATABASE/COVID19 NEGATIVE/Tables/SES_Negative_info_Seed1.csv\",sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frontal vs Lateral Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SES_df=pd.read_csv(\"/media/ia/DATA/COVID19/SES_DATABASE/COVID19 NEGATIVE/Tables/SES_Negative_info_Seed1.csv\").drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SES_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datos para aplicar normalización y estandartización\n",
    "X_train_F = np.load('/media/ia/DATA/COVID19/NPY_Data/COVID/X_train_COVID_1CH.npy')\n",
    "X_L = np.load('/media/ia/DATA/COVID19/NPY_Data/COVID/X_test_lateral.npy')\n",
    "X_train_L,X_test_L,X_dev_L = np.array_split(X_L,[375,611])\n",
    "X_train=np.concatenate((X_train_F,X_train_L))\n",
    "\n",
    "X_train=min_max_preprocessing(X_train)\n",
    "\n",
    "X_train,mean,std=samplewise_preprocessing(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_VGG19_gray():\n",
    "      model = tf.keras.applications.VGG19(weights='imagenet', include_top=False)\n",
    "\n",
    "      # Block1_conv1 weights are of the format [3, 3, 3, 64] -> this is for RGB images\n",
    "      # For grayscale, format should be [3, 3, 1, 64]. Weighted average of the features has to be calculated across channels.\n",
    "      # RGB weights: Red 0.2989, Green 0.5870, Blue 0.1140\n",
    "\n",
    "      # getting weights of block1 conv1.\n",
    "      block1_conv1 = model.get_layer('block1_conv1').get_weights()\n",
    "      weights, biases = block1_conv1\n",
    "\n",
    "      # :weights shape = [3, 3, 3, 64] - (0, 1, 2, 3)\n",
    "      # convert :weights shape to = [64, 3, 3, 3] - (3, 2, 0, 1)\n",
    "      weights = np.transpose(weights, (3, 2, 0, 1))\n",
    "\n",
    "\n",
    "      kernel_out_channels, kernel_in_channels, kernel_rows, kernel_columns = weights.shape\n",
    "\n",
    "      # Dimensions : [kernel_out_channels, 1 (since grayscale), kernel_rows, kernel_columns]\n",
    "      grayscale_weights = np.zeros((kernel_out_channels, 1, kernel_rows, kernel_columns))\n",
    "\n",
    "      # iterate out_channels number of times\n",
    "      for i in range(kernel_out_channels):\n",
    "\n",
    "        # get kernel for every out_channel\n",
    "        get_kernel = weights[i, :, :, :]\n",
    "\n",
    "        temp_kernel = np.zeros((3, 3))\n",
    "\n",
    "        # :get_kernel shape = [3, 3, 3]\n",
    "        # axis, dims = (0, in_channel), (1, row), (2, col)\n",
    "\n",
    "        # calculate weighted average across channel axis\n",
    "        in_channels, in_rows, in_columns = get_kernel.shape\n",
    "\n",
    "        for in_row in range(in_rows):\n",
    "          for in_col in range(in_columns):\n",
    "            feature_red = get_kernel[0, in_row, in_col]\n",
    "            feature_green = get_kernel[1, in_row, in_col]\n",
    "            feature_blue = get_kernel[2, in_row, in_col]\n",
    "\n",
    "            # weighted average for RGB filter\n",
    "            total = (feature_red * 0.2989) + (feature_green * 0.5870) + (feature_blue * 0.1140)\n",
    "\n",
    "            temp_kernel[in_row, in_col] = total\n",
    "\n",
    "\n",
    "        # :temp_kernel is a 3x3 matrix [rows x columns]\n",
    "        # add an axis at the end to specify in_channel as 1\n",
    "\n",
    "        # Second: Add axis at the start of :temp_kernel to make its shape: [1, 3, 3] which is [in_channel, rows, columns]\n",
    "        temp_kernel = np.expand_dims(temp_kernel, axis=0)\n",
    "\n",
    "        # Now, :temp_kernel shape is [1, 3, 3]\n",
    "\n",
    "        # Concat :temp_kernel to :grayscale_weights along axis=0\n",
    "        grayscale_weights[i, :, :, :] = temp_kernel\n",
    "\n",
    "      # Dimension of :grayscale_weights is [64, 1, 3, 3]\n",
    "      # In order to bring it to tensorflow or keras weight format, transpose :grayscale_weights\n",
    "\n",
    "      # dimension, axis of :grayscale_weights = (out_channels: 0), (in_channels: 1), (rows: 2), (columns: 3)\n",
    "      # tf format of weights = (rows: 0), (columns: 1), (in_channels: 2), (out_channels: 3)\n",
    "\n",
    "      # Go from (0, 1, 2, 3) to (2, 3, 1, 0)\n",
    "      grayscale_weights = np.transpose(grayscale_weights, (2, 3, 1, 0)) # (3, 3, 1, 64)\n",
    "\n",
    "      # combine :grayscale_weights and :biases\n",
    "      new_block1_conv1 = [grayscale_weights, biases]\n",
    "\n",
    "\n",
    "      # Reconstruct the layers of VGG16 but replace block1_conv1 weights with :grayscale_weights\n",
    "\n",
    "      # get weights of all the layers starting from 'block1_conv2'\n",
    "      vgg19_weights = {}\n",
    "      for layer in model.layers[2:]:\n",
    "        if \"conv\" in layer.name:\n",
    "          vgg19_weights[\"224_\" + layer.name] = model.get_layer(layer.name).get_weights()\n",
    "\n",
    "      del model\n",
    "\n",
    "\n",
    "      # Custom build VGG19\n",
    "      input = Input(shape=(224, 224, 1), name='224_input')\n",
    "      # Block 1\n",
    "      x = Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(224, 224, 1), data_format=\"channels_last\", name='224_block1_conv1')(input)\n",
    "      x = Conv2D(64, (3, 3), activation='relu', padding='same', name='224_block1_conv2')(x)\n",
    "      x = MaxPooling2D((2, 2), strides=(2, 2), name='224_block1_pool')(x)\n",
    "\n",
    "      # Block 2\n",
    "      x = Conv2D(128, (3, 3), activation='relu', padding='same', name='224_block2_conv1')(x)\n",
    "      x = Conv2D(128, (3, 3), activation='relu', padding='same', name='224_block2_conv2')(x)\n",
    "      x = MaxPooling2D((2, 2), strides=(2, 2), name='224_block2_pool')(x)\n",
    "\n",
    "      # Block 3\n",
    "      x = Conv2D(256, (3, 3), activation='relu', padding='same', name='224_block3_conv1')(x)\n",
    "      x = Conv2D(256, (3, 3), activation='relu', padding='same', name='224_block3_conv2')(x)\n",
    "      x = Conv2D(256, (3, 3), activation='relu', padding='same', name='224_block3_conv3')(x)\n",
    "      x = Conv2D(256, (3, 3), activation='relu', padding='same', name='224_block3_conv4')(x)\n",
    "      x = MaxPooling2D((2, 2), strides=(2, 2), name='224_block3_pool')(x)\n",
    "\n",
    "      # Block 4\n",
    "      x = Conv2D(512, (3, 3), activation='relu', padding='same', name='224_block4_conv1')(x)\n",
    "      x = Conv2D(512, (3, 3), activation='relu', padding='same', name='224_block4_conv2')(x)\n",
    "      x = Conv2D(512, (3, 3), activation='relu', padding='same', name='224_block4_conv3')(x)\n",
    "      x = Conv2D(512, (3, 3), activation='relu', padding='same', name='224_block4_conv4')(x)\n",
    "      x = MaxPooling2D((2, 2), strides=(2, 2), name='224_block4_pool')(x)\n",
    "\n",
    "      # Block 5\n",
    "      x = Conv2D(512, (3, 3), activation='relu', padding='same', name='224_block5_conv1')(x)\n",
    "      x = Conv2D(512, (3, 3), activation='relu', padding='same', name='224_block5_conv2')(x)\n",
    "      x = Conv2D(512, (3, 3), activation='relu', padding='same', name='224_block5_conv3')(x)\n",
    "      x = Conv2D(512, (3, 3), activation='relu', padding='same', name='224_block5_conv4')(x)\n",
    "      x = MaxPooling2D((8, 8), strides=(8, 8), name='224_block5_pool')(x)\n",
    "\n",
    "      base_model = Model(inputs=input, outputs=x)\n",
    "\n",
    "      base_model.get_layer('224_block1_conv1').set_weights(new_block1_conv1)\n",
    "      for layer in base_model.layers[2:]:\n",
    "        if 'conv' in layer.name:\n",
    "          base_model.get_layer(layer.name).set_weights(vgg19_weights[layer.name])\n",
    "\n",
    "      x = base_model.output\n",
    "\n",
    "      for layer in base_model.layers:\n",
    "          layer.trainable = True\n",
    "\n",
    "      x = tf.keras.layers.GlobalAveragePooling2D()(x)  \n",
    "      layers = tf.keras.layers.Flatten()(x)\n",
    "      #layers = tf.keras.layers.Dense(128,activation=\"relu\")(layers)\n",
    "      layers = tf.keras.layers.Dropout(0.2)(layers)\n",
    "      layers = tf.keras.layers.Dense(1024 ,activation=\"relu\")(layers)\n",
    "      layers = tf.keras.layers.Dropout(0.2)(layers)\n",
    "      layers = tf.keras.layers.Dense( 512,activation=\"relu\")(layers)\n",
    "      layers = tf.keras.layers.Dense( 64,activation=\"relu\")(layers)\n",
    "      predictions = tf.keras.layers.Dense(2, activation=\"softmax\", name=\"output_1\")(layers)\n",
    "\n",
    "      #Compilador\n",
    "      model = tf.keras.Model(inputs = base_model.input, outputs=predictions)\n",
    "      optimizer=tf.keras.optimizers.Adam(lr=0.0001) \n",
    "      model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "      model.summary()\n",
    "      return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_VGG19_gray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"/media/ia/DATA/COVID19/VGG19_For_Covid_FyL.h5\") #Load weights to Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(SES_df))): \n",
    "    PATH='/media/ia/DATA/COVID19/Negativas XNAT/p0032021/'\n",
    "    img,ph=dicom2numpy(PATH+SES_df['filepath'][i])\n",
    "    if ph=='MONOCHROME1':\n",
    "        img=-img\n",
    "    img=cv2.resize(img, (224, 224))\n",
    "    maxi=np.max(img)\n",
    "    mini=np.min(img)\n",
    "    img_N=(img-mini)/(maxi-mini)\n",
    "    img_P=(img_N-mean)/std\n",
    "    \n",
    "    predictions=model.predict(np.expand_dims(img_P,axis=0))\n",
    "    y_pred_bool = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    if y_pred_bool[0]==0:\n",
    "        SES_df['projection'][i]='F'\n",
    "    if y_pred_bool[0]==1:\n",
    "        SES_df['projection'][i]='NA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SES_df.loc[SES_df.projection.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img,ph=dicom2numpy(PATH+SES_df.loc[SES_df.projection.isna()]['filepath'][2041])\n",
    "plt.imshow(img,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SES_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Images Using 3 Seeds (2,4,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SES_Frontal=SES_df.loc[SES_df.projection=='F'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SES_Frontal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img,ph=dicom2numpy(PATH+SES_Frontal['filepath'][500])\n",
    "if ph=='MONOCHROME1':\n",
    "    img=-img\n",
    "img=cv2.resize(img, (224, 224))\n",
    "plt.imshow(img,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='/media/ia/DATA/COVID19/Negativas XNAT/p0032021/'\n",
    "destination_f=\"/media/ia/DATA/COVID19/SES_DATABASE/COVID19 NEGATIVE/NPY Processed/Normal Images/Complete/\"\n",
    "\n",
    "saveNPY_from_DICOM(SES_Frontal,destination_f, 'SES_COVID_NEGATIVE_224_Normal',PATH,'filepath',W=224,H=224,C_Labels=False,rmv_black_borders=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_SES_P=np.load(\"/media/ia/DATA/COVID19/SES_DATABASE/COVID19 NEGATIVE/NPY Processed/Normal Images/Complete/X_SES_COVID_NEGATIVE_224_Normal.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_SES_P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_SES_P[5],cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# División train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_dict(p):\n",
    "    dti = dict()\n",
    "    path = p.replace(PATH, \"\").split(\"/\")\n",
    "    modality = path[-1].replace(\".png\", \"\").split(\"_\")[-1]    \n",
    "    dti['Subject'] = path[0]\n",
    "    dti['Session'] = path[1]\n",
    "    dti['mod'] = path[2]\n",
    "    dti['File'] = path[3]\n",
    "    dti['Type'] = modality\n",
    "    dti['Path'] = p\n",
    "    json_file = p.replace('.png', '.json').replace('.nii.gz', '.json')\n",
    "    # If there is no json associated with the file, then pick the json on the series. \n",
    "    # Maybe multiple acquisitions with the same parameters in the same series?\n",
    "    if not os.path.exists(json_file):\n",
    "        json_in_series = glob.glob(os.path.join(PATH, path[0], path[1], path[2], '*.json'))\n",
    "        if len(json_in_series) >= 1:\n",
    "            json_file = json_in_series[0]\n",
    "    if os.path.exists(json_file):\n",
    "        with open(json_file) as f:\n",
    "            dtij = json.load(f)\n",
    "        for l in dtij.keys():\n",
    "            if 'Value' in dtij[l].keys():\n",
    "                try:\n",
    "                    dti[pydicom.datadict.dictionary_description(str(l))] = dtij[l]['Value']\n",
    "                except:\n",
    "                    dti[str(l)] = dtij[l]['Value']\n",
    "    else:\n",
    "        print(p)\n",
    "    return dti\n",
    "\n",
    "def min_max_preprocessing(images):\n",
    "    processed_images = []\n",
    "    for i in range(len(images)):\n",
    "        maxi=np.max(images[i])\n",
    "        mini=np.min(images[i])\n",
    "        processed_images.append((images[i]-mini)/(maxi-mini))\n",
    "    return np.array(processed_images)\n",
    "\n",
    "def samplewise_preprocessing(images):\n",
    "    processed_images = []\n",
    "    means = []\n",
    "    stds = []\n",
    "    for i in range(images.shape[0]):\n",
    "        mean = np.mean(images[i])\n",
    "        std = np.std(images[i])\n",
    "        if std!=0 and mean != 0:\n",
    "            means.append(mean)\n",
    "            stds.append(std)\n",
    "            processed_images.append((images[i]-mean)/std)\n",
    "    \n",
    "    return np.array(processed_images), np.mean(means), np.mean(stds)\n",
    "\n",
    "\n",
    "def featurewise_preprocessing(images, mean, std):\n",
    "    processed_images = np.zeros_like(images, dtype=np.float32)\n",
    "    for i in range(images.shape[0]):\n",
    "        processed_images[i] = (images[i]-mean)/std\n",
    "    return processed_images\n",
    "\n",
    "def checkDuplicates(trainDF,devDF,testDF,id_column):\n",
    "    patientsTrain = set(trainDF[id_column])\n",
    "    patientsDev = set(devDF[id_column])\n",
    "    patientsTest = set(testDF[id_column])\n",
    "\n",
    "    ids = list(patientsTrain.intersection(patientsDev))\n",
    "    print('# de pacientes de train presentes en dev:', len(ids))\n",
    "\n",
    "    ids_ = list(patientsTrain.intersection(patientsTest))\n",
    "    print('# de pacientes de train presentes en test:', len(ids_))\n",
    "    ids.extend(ids_)\n",
    "\n",
    "    ids_dev = list(patientsDev.intersection(patientsTest))\n",
    "    print('# de pacientes de dev presentes en test:', len(ids_dev))\n",
    "\n",
    "def remove_black_borders_1(img):\n",
    "    rows, cols = img.shape\n",
    "    start_row, end_row = 0, 0\n",
    "    start_col, end_col = 0, 0\n",
    "    \n",
    "    for i in range(rows):\n",
    "        if sum(img[i, :]) > 0:\n",
    "            start_row = i\n",
    "            break\n",
    "    for i in reversed(range(rows)):\n",
    "        if (sum(img[i, :]) > 0):\n",
    "            end_row = i\n",
    "            break    \n",
    "    for i in range(cols):\n",
    "        if (sum(img[:, i]) > 0):\n",
    "            start_col = i\n",
    "            break\n",
    "    for i in reversed(range(cols)):\n",
    "        if (sum(img[:, i]) > 0):\n",
    "            end_col = i\n",
    "            break\n",
    "   \n",
    "    return img[start_row:end_row, start_col:end_col]\n",
    "    \n",
    "\n",
    "def saveNPY(DF,destination, name,path,src_column,W=224,H=224,C_Labels=False,rmv_black_borders=False):\n",
    "    src_dir = path\n",
    "    images = []\n",
    "\n",
    "    print('reading images...')\n",
    "\n",
    "    for i in tqdm(range(len(DF))):\n",
    "        src_file = os.path.join(src_dir, DF[src_column][i])\n",
    "        img = cv2.imread(src_file,-1)\n",
    "        dti=json_to_dict(DF['filepath'][i])\n",
    "        if dti['00280004'][0]=='MONOCHROME1':\n",
    "            img=-img\n",
    "        if rmv_black_borders:\n",
    "            try:\n",
    "                img=remove_black_borders_1(img)\n",
    "            except:\n",
    "                continue\n",
    "        resized = cv2.resize(img, (W, H))\n",
    "        if resized.shape==(W,H,4):\n",
    "            images.append(resized[:,:,0])\n",
    "        else:\n",
    "            images.append(resized)\n",
    "\n",
    "    NPY = np.array(images)\n",
    "    images_filename = destination+'X_'+name+'.npy'\n",
    "    np.save(images_filename, NPY)\n",
    "    if C_Labels:\n",
    "\n",
    "        labels_ = DF.group.replace(['C', 'N', 'I', 'NI'], [0, 1, 2, 3])\n",
    "        labels = tf.keras.utils.to_categorical(labels_, num_classes=4)\n",
    "  \n",
    "        labels_filename = destination+'/y_'+name+'.npy'\n",
    "        np.save(labels_filename, labels)\n",
    " \n",
    "    \n",
    "\n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Partitions=pd.read_csv(\"/media/ia/DATA/COVID19/SES_DATABASE/COVID19 NEGATIVE/Tables/SES_Negative_info_Seed3.csv\").drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(8)#3 Seeds (2,4,8)\n",
    "data=Partitions.loc[Partitions.projection=='F']\n",
    "a=data['subject'].unique()\n",
    "train=np.random.choice(a,size=int(len(a)*1.18))\n",
    "data1=data.drop(data.loc[data.subject.isin(train)].index)\n",
    "b=data1['subject'].unique()\n",
    "dev=np.random.choice(b,size=int(len(b)*0.85))\n",
    "test=data1.drop(data1.loc[data1.subject.isin(dev)].index)['subject'].unique()\n",
    "\n",
    "#test=data.drop(data.loc[data.subject.isin(train)].index)['subject'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF_SES=data.loc[data.subject.isin(train)]\n",
    "devDF_SES=data.loc[data.subject.isin(dev)]\n",
    "testDF_SES=data.loc[data.subject.isin(test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF_SES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF_SES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devDF_SES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkDuplicates(trainDF_SES,devDF_SES,testDF_SES,'subject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cantidad imagenes:',len(data))\n",
    "print('Cantidad imagenes train:',len(trainDF_SES))\n",
    "print('Cantidad imagenes dev: ',len(devDF_SES))\n",
    "print('Cantidad imagenes test:',len(testDF_SES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Partitions['partition'][Partitions.subject.isin(train)]='tr'\n",
    "Partitions['partition'][Partitions.subject.isin(dev)]='dev'\n",
    "Partitions['partition'][Partitions.subject.isin(test)]='te'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Partitions.to_csv(\"/media/ia/DATA/COVID19/SES_DATABASE/COVID19 NEGATIVE/Tables/SES_Negative_info_Seed3.csv\",sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF_SES.to_csv(\"/media/ia/DATA/COVID19/SES_DATABASE/COVID19 NEGATIVE/Tables/Train, Validation, Test/Seed3/Info_Neg_Train_Seed3.tsv\",sep='\\t')\n",
    "devDF_SES.to_csv(\"/media/ia/DATA/COVID19/SES_DATABASE/COVID19 NEGATIVE/Tables/Train, Validation, Test/Seed3/Info_Neg_Dev_Seed3.tsv\",sep='\\t')\n",
    "testDF_SES.to_csv(\"/media/ia/DATA/COVID19/SES_DATABASE/COVID19 NEGATIVE/Tables/Train, Validation, Test/Seed3/Info_Neg_Test_Seed3.tsv\",sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF=pd.read_csv(\"/media/ia/DATA/COVID19/SES_DATABASE/COVID19 NEGATIVE/Tables/Train, Validation, Test/Seed3/Info_Neg_Train_Seed3.tsv\",sep='\\t').drop(['Unnamed: 0'],axis=1)\n",
    "devDF=pd.read_csv(\"/media/ia/DATA/COVID19/SES_DATABASE/COVID19 NEGATIVE/Tables/Train, Validation, Test/Seed3/Info_Neg_Dev_Seed3.tsv\",sep='\\t').drop(['Unnamed: 0'],axis=1)\n",
    "testDF=pd.read_csv(\"/media/ia/DATA/COVID19/SES_DATABASE/COVID19 NEGATIVE/Tables/Train, Validation, Test/Seed3/Info_Neg_Test_Seed3.tsv\",sep='\\t').drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='/media/ia/DATA/COVID19/Negativas XNAT/p0032021/'\n",
    "destination_f=\"/media/ia/DATA/COVID19/SES_DATABASE/COVID19 NEGATIVE/NPY Processed/Normal Images/Complete/Train, Validation, Test/600x600/Seed3/\"\n",
    "\n",
    "saveNPY_from_DICOM(trainDF,destination_f, 'Neg_Train_600_Seed3',PATH,'filepath',W=600,H=600,C_Labels=False,rmv_black_borders=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='/media/ia/DATA/COVID19/Negativas XNAT/p0032021/'\n",
    "destination_f=\"/media/ia/DATA/COVID19/SES_DATABASE/COVID19 NEGATIVE/NPY Processed/Normal Images/Complete/Train, Validation, Test/600x600/Seed3/\"\n",
    "\n",
    "saveNPY_from_DICOM(devDF,destination_f, 'Neg_Dev_600_Seed3',PATH,'filepath',W=600,H=600,C_Labels=False,rmv_black_borders=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='/media/ia/DATA/COVID19/Negativas XNAT/p0032021/'\n",
    "destination_f=\"/media/ia/DATA/COVID19/SES_DATABASE/COVID19 NEGATIVE/NPY Processed/Normal Images/Complete/Train, Validation, Test/600x600/Seed3/\"\n",
    "\n",
    "saveNPY_from_DICOM(testDF,destination_f, 'Neg_Test_600_Seed3',PATH,'filepath',W=600,H=600,C_Labels=False,rmv_black_borders=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='/media/ia/DATA/COVID19/Negativas XNAT/p0032021/'\n",
    "destination_f=\"/media/ia/DATA/COVID19/SES_DATABASE/COVID19 NEGATIVE/NPY Processed/Normal Images/Complete/Train, Validation, Test/224x224/Seed3/\"\n",
    "\n",
    "saveNPY_from_DICOM(trainDF,destination_f, 'Neg_Train_224_Seed3',PATH,'filepath',W=224,H=224,C_Labels=False,rmv_black_borders=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='/media/ia/DATA/COVID19/Negativas XNAT/p0032021/'\n",
    "destination_f=\"/media/ia/DATA/COVID19/SES_DATABASE/COVID19 NEGATIVE/NPY Processed/Normal Images/Complete/Train, Validation, Test/224x224/Seed3/\"\n",
    "\n",
    "saveNPY_from_DICOM(devDF,destination_f, 'Neg_Dev_224_Seed3',PATH,'filepath',W=224,H=224,C_Labels=False,rmv_black_borders=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='/media/ia/DATA/COVID19/Negativas XNAT/p0032021/'\n",
    "destination_f=\"/media/ia/DATA/COVID19/SES_DATABASE/COVID19 NEGATIVE/NPY Processed/Normal Images/Complete/Train, Validation, Test/224x224/Seed3/\"\n",
    "\n",
    "saveNPY_from_DICOM(testDF,destination_f, 'Neg_Test_224_Seed3',PATH,'filepath',W=224,H=224,C_Labels=False,rmv_black_borders=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_SES=np.load(destination_f+\"X_Neg_Dev_224_Seed3.npy\")\n",
    "print(X_SES.shape)\n",
    "plt.imshow(X_SES[1],cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
